{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chlwnsgur129/anaconda3/envs/ga/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/chlwnsgur129/anaconda3/envs/ga/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/chlwnsgur129/anaconda3/envs/ga/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torchvision.transforms import CenterCrop, Resize\n",
    "from PIL import Image\n",
    "\n",
    "from swinmodel import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':100,\n",
    "    'LEARNING_RATE':2e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "training_base_dir = os.path.join(data_dir, 'Training')\n",
    "validation_base_dir = os.path.join(data_dir, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, clean_image_paths, noisy_image_paths, transform=None):\n",
    "        self.clean_image_paths = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
    "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "        self.center_crop = CenterCrop(1080)\n",
    "        self.resize = Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "\n",
    "        # Create a list of (noisy, clean) pairs\n",
    "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
    "\n",
    "    def _create_noisy_clean_pairs(self):\n",
    "        clean_to_noisy = {}\n",
    "        for clean_path in self.clean_image_paths:\n",
    "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
    "            clean_to_noisy[clean_id] = clean_path\n",
    "        \n",
    "        noisy_clean_pairs = []\n",
    "        for noisy_path in self.noisy_image_paths:\n",
    "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
    "            if noisy_id in clean_to_noisy:\n",
    "                clean_path = clean_to_noisy[noisy_id]\n",
    "                noisy_clean_pairs.append((noisy_path, clean_path))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return noisy_clean_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_clean_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
    "\n",
    "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
    "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Central Crop and Resize\n",
    "        noisy_image = self.center_crop(noisy_image)\n",
    "        clean_image = self.center_crop(clean_image)\n",
    "        noisy_image = self.resize(noisy_image)\n",
    "        clean_image = self.resize(clean_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            clean_image = self.transform(clean_image)\n",
    "        \n",
    "        return noisy_image, clean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 데이터셋 경로\n",
    "noisy_image_paths = './data/Training/noisy'\n",
    "clean_image_paths = './data/Training/clean'\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "train_transform = Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "#train_transform = transforms.Compose([\n",
    "#    transforms.RandomCrop(128),\n",
    "#    transforms.RandomHorizontalFlip(),\n",
    "#    transforms.RandomVerticalFlip(),\n",
    "#    transforms.RandomRotation(90),\n",
    "#    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.1)\n",
    "#])\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.RandomCrop(128),\n",
    "    #transforms.RandomHorizontalFlip(p=0.2),\n",
    "    #transforms.RandomVerticalFlip(p=0.2),\n",
    "    #transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 커스텀 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "num_cores = os.cpu_count()\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "print(device)\n",
    "# Restormer 모델 인스턴스 생성 및 GPU로 이동\n",
    "#model = Restormer().to(device)\n",
    "model = SwinIR(img_size=64, \n",
    "               patch_size=1, \n",
    "               in_chans=3,\n",
    "               embed_dim=96, \n",
    "               depths=[6, 6, 6, 6], \n",
    "               num_heads=[6, 6, 6, 6], \n",
    "               window_size=7, \n",
    "               mlp_ratio=2, \n",
    "               upsampler='pixelshuffledirect').to(device)\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0,1,2,3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda import memory_allocated, max_memory_allocated\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_ssim\n",
    "\n",
    "# 커스텀 손실 함수 정의\n",
    "def combined_loss(output, target):\n",
    "    l1_loss = nn.L1Loss()(output, target)\n",
    "    ssim_loss = 1 - pytorch_ssim.ssim(output, target)\n",
    "    return l1_loss + ssim_loss * 0.1  # 가중치는 필요에 따라 조절\n",
    "\n",
    "# 손실 함수 업데이트\n",
    "#criterion = combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2247804\n"
     ]
    }
   ],
   "source": [
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "#optimizer = optim.Adam(model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "scaler = GradScaler()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "# 모델의 파라미터 수 계산\n",
    "total_parameters = count_parameters(model)\n",
    "print(\"Total Parameters:\", total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2247804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, MSE Loss: 0.0555, PSNR: 12.63 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 16분 1초\n",
      "1epoch 모델 저장 완료 (PSNR: 12.63 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, MSE Loss: 0.0542, PSNR: 12.72 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 15분 48초\n",
      "2epoch 모델 저장 완료 (PSNR: 12.72 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, MSE Loss: 0.0535, PSNR: 12.79 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 15분 48초\n",
      "3epoch 모델 저장 완료 (PSNR: 12.79 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, MSE Loss: 0.0534, PSNR: 12.79 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 16분 8초\n",
      "4epoch 모델 저장 완료 (PSNR: 12.79 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, MSE Loss: 0.0531, PSNR: 12.82 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 15분 51초\n",
      "5epoch 모델 저장 완료 (PSNR: 12.82 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, MSE Loss: 0.0528, PSNR: 12.84 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 15분 47초\n",
      "6epoch 모델 저장 완료 (PSNR: 12.84 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, MSE Loss: 0.0528, PSNR: 12.85 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 15분 56초\n",
      "7epoch 모델 저장 완료 (PSNR: 12.85 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, MSE Loss: 0.0524, PSNR: 12.88 dB, Lr: 0.00010000\n",
      "1epoch 훈련 소요 시간: 0시간 15분 53초\n",
      "8epoch 모델 저장 완료 (PSNR: 12.88 dB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100:  85%|████████▍ | 740/871 [14:53<02:44,  1.25s/it, Batch Loss=0.0456, PSNR=13.4, LR=0.0001]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the configuration (example CFG)\n",
    "\n",
    "# Function to count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate and print model parameters\n",
    "total_parameters = count_parameters(model)\n",
    "print(\"Total Parameters:\", total_parameters)\n",
    "\n",
    "# PSNR calculation function\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 20 * math.log10(1.0 / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "# Model training\n",
    "model.train()\n",
    "best_loss = 1000\n",
    "best_psnr = 0\n",
    "\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "    mse_running_loss = 0.0\n",
    "    psnr_running = 0.0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG['EPOCHS']}\", leave=False) as pbar:\n",
    "        for batch_idx, (noisy_images, clean_images) in enumerate(pbar):\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            clean_images = clean_images.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(noisy_images)\n",
    "                # Ensure output size matches the clean image size\n",
    "                outputs = F.interpolate(outputs, size=clean_images.shape[2:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Calculate MSE loss\n",
    "                mse_loss = criterion(outputs, clean_images)\n",
    "            \n",
    "            # Scale the loss for mixed precision\n",
    "            scaler.scale(mse_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Accumulate the running loss\n",
    "            mse_running_loss += mse_loss.item() * noisy_images.size(0)\n",
    "            \n",
    "            # Calculate PSNR\n",
    "            psnr = calculate_psnr(outputs, clean_images)\n",
    "            psnr_running += psnr * noisy_images.size(0)\n",
    "\n",
    "            # Update progress bar with current batch stats\n",
    "            pbar.set_postfix({\n",
    "                'Batch Loss': mse_loss.item(),\n",
    "                'PSNR': psnr,\n",
    "                'LR': optimizer.param_groups[0]['lr']  # Getting LR directly from optimizer\n",
    "            })\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    hours = int(minutes // 60)\n",
    "    minutes = int(minutes % 60)\n",
    "\n",
    "    # Calculate epoch-level average loss and PSNR\n",
    "    mse_epoch_loss = mse_running_loss / len(train_loader.dataset)\n",
    "    psnr_epoch = psnr_running / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, MSE Loss: {mse_epoch_loss:.4f}, PSNR: {psnr_epoch:.2f} dB, Lr: {current_lr:.8f}\")\n",
    "    print(f\"1epoch 훈련 소요 시간: {hours}시간 {minutes}분 {seconds}초\")\n",
    "\n",
    "    # Step the scheduler with epoch-level average loss\n",
    "    scheduler.step(mse_epoch_loss)\n",
    "    \n",
    "    # Save model if PSNR is the best so far\n",
    "    if psnr_epoch > best_psnr:\n",
    "        best_psnr = psnr_epoch\n",
    "        torch.save(model.state_dict(), 'b16_noarg.pth')\n",
    "        print(f\"{epoch+1}epoch 모델 저장 완료 (PSNR: {best_psnr:.2f} dB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2247804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(outputs, size\u001b[38;5;241m=\u001b[39mclean_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m     mse_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, clean_images)\n",
      "File \u001b[0;32m~/anaconda3/envs/ga/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ga/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ga/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:172\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m inputs, module_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscatter(inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:3"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "#optimizer = optim.Adam(model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "#criterion = nn.L1Loss()\n",
    "#scaler = GradScaler()\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
    "\n",
    "# 모델의 파라미터 수 계산\n",
    "total_parameters = count_parameters(model)\n",
    "print(\"Total Parameters:\", total_parameters)\n",
    "\n",
    "\n",
    "# PSNR 계산 함수 정의\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 20 * math.log10(1.0 / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "# 모델 학습\n",
    "model.train()\n",
    "best_loss = 1000\n",
    "best_psnr = 0\n",
    "\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "    mse_running_loss = 0.0\n",
    "    psnr_running = 0.0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG['EPOCHS']}\", leave=False) as pbar:\n",
    "        for batch_idx, (noisy_images, clean_images) in enumerate(pbar):\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            clean_images = clean_images.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(noisy_images)\n",
    "                outputs = F.interpolate(outputs, size=clean_images.shape[2:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                mse_loss = criterion(outputs, clean_images)\n",
    "            \n",
    "            scaler.scale(mse_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            mse_running_loss += mse_loss.item() * noisy_images.size(0)\n",
    "            \n",
    "            # PSNR 계산\n",
    "            psnr = calculate_psnr(outputs, clean_images)\n",
    "            psnr_running += psnr * noisy_images.size(0)\n",
    "\n",
    "            # 진행 상황 업데이트\n",
    "            pbar.set_postfix({\n",
    "                'Batch Loss': mse_loss.item(),\n",
    "                'PSNR': psnr,\n",
    "                'LR': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    hours = int(minutes // 60)\n",
    "    minutes = int(minutes % 60)\n",
    "\n",
    "    # Epoch 단위 평균 손실 및 PSNR 계산\n",
    "    mse_epoch_loss = mse_running_loss / len(train_dataset)\n",
    "    psnr_epoch = psnr_running / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, MSE Loss: {mse_epoch_loss:.4f}, PSNR: {psnr_epoch:.2f} dB, Lr: {current_lr:.8f}\")\n",
    "    print(f\"1epoch 훈련 소요 시간: {hours}시간 {minutes}분 {seconds}초\")\n",
    "\n",
    "    # PSNR이 최고일 때 모델 저장\n",
    "    if psnr_epoch > best_psnr:\n",
    "        best_psnr = psnr_epoch\n",
    "        torch.save(model.state_dict(), 'best_Swin_psnr.pth')\n",
    "        print(f\"{epoch+1}epoch 모델 저장 완료 (PSNR: {best_psnr:.2f} dB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetTest(data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, transform=None):\n",
    "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(self.noisy_image_paths[index])\n",
    "        \n",
    "        # Convert numpy array to PIL image\n",
    "        if isinstance(noisy_image, np.ndarray):\n",
    "            noisy_image = Image.fromarray(noisy_image)\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path\n",
    "\n",
    "\n",
    "test_transform = Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')\n",
    "print(device)\n",
    "# Restormer 모델 인스턴스 생성 및 GPU로 이동\n",
    "#model = Restormer()#.to(device)\n",
    "model = SwinIR(img_size=64, \n",
    "               patch_size=1, \n",
    "               in_chans=3,\n",
    "               embed_dim=96, \n",
    "               depths=[6, 6, 6, 6], \n",
    "               num_heads=[6, 6, 6, 6], \n",
    "               window_size=7, \n",
    "               mlp_ratio=2, \n",
    "               upsampler='pixelshuffledirect')\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0,3]).to(device)\n",
    "# State dict 불러오기\n",
    "state_dict = torch.load('best_Restormer1.pth')\n",
    "\n",
    "# \"module.\" 제거\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith(\"module.\") else k  # \"module.\" 제거\n",
    "    new_state_dict[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinIR(img_size=64, \n",
    "               patch_size=1, \n",
    "               in_chans=3,\n",
    "               embed_dim=96, \n",
    "               depths=[6, 6, 6, 6], \n",
    "               num_heads=[6, 6, 6, 6], \n",
    "               window_size=7, \n",
    "               mlp_ratio=2, \n",
    "               upsampler='pixelshuffledirect')\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# State dict 불러오기 후 \"module.\" 제거\n",
    "state_dict = torch.load('best_Restormer1.pth')\n",
    "new_state_dict = OrderedDict((k[7:] if k.startswith(\"module.\") else k, v) for k, v in state_dict.items())\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "test_data_path = './data/Validation/noisy'\n",
    "output_path = './data/Validation/output'\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "test_dataset = CustomDatasetTest(test_data_path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# 이미지 denoising 및 저장\n",
    "for noisy_image, noisy_image_path in test_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    with torch.no_grad():\n",
    "        denoised_image = model(noisy_image)\n",
    "    \n",
    "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = (denoised_image * 0.5 + 0.5).clamp(0, 1)\n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # 저장 경로 지정 및 저장\n",
    "    output_filename = noisy_image_path[0] if isinstance(noisy_image_path, list) else noisy_image_path\n",
    "    print(output_filename)\n",
    "    denoised_filename = output_path + '/' + output_filename[0].split('/')[-1][:-4] + '.jpg'\n",
    "    denoised_image.save(denoised_filename)\n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SwinIR(img_size=64, \n",
    "               patch_size=1, \n",
    "               in_chans=3,\n",
    "               embed_dim=96, \n",
    "               depths=[6, 6, 6, 6], \n",
    "               num_heads=[6, 6, 6, 6], \n",
    "               window_size=7, \n",
    "               mlp_ratio=2, \n",
    "               upsampler='pixelshuffledirect')\n",
    "\n",
    "# State dict 불러오기\n",
    "state_dict = torch.load('best_Restormer1.pth')\n",
    "\n",
    "# \"module.\" 제거\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith(\"module.\") else k  # \"module.\" 제거\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# 모델에 새로운 state dict 로드\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:3')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "test_data_path = './data/Validation/noisy'\n",
    "output_path = './data/Validation/output'\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "test_dataset = CustomDatasetTest(test_data_path, transform=test_transform)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# 이미지 denoising 및 저장\n",
    "for noisy_image, noisy_image_path in test_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    denoised_image = model(noisy_image)\n",
    "    \n",
    "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = (denoised_image * 0.5 + 0.5).clamp(0, 1)\n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # Save denoised image\n",
    "    output_filename = noisy_image_path[0]\n",
    "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.jpg'\n",
    "    denoised_image.save(denoised_filename) \n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_folder(folder_path, output_zip):\n",
    "    shutil.make_archive(output_zip, 'zip', folder_path)\n",
    "    print(f\"Created {output_zip}.zip successfully.\")\n",
    "\n",
    "zip_folder(output_path, './submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
